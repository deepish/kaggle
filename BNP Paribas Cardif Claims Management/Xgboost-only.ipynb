{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time; start_time=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss,make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random; random.seed(2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_train = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save target from train in another series and Drop the target variable \n",
    "y_train = train['target']\n",
    "train = train.drop('target', axis=1)\n",
    "id_test = test['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merge the train and test data to perform operations on the data\n",
    "df_all = pd.concat((train,test),axis=0,ignore_index=True)\n",
    "df_all['null_count'] = df_all.isnull().sum(axis=1).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Drop the ids\n",
    "df_all_temp = df_all['ID']\n",
    "df_all = df_all.drop('ID', axis=1)\n",
    "df_data_types = df_all.dtypes[:]\n",
    "\n",
    "d_col_drops = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_nan_null(val):\n",
    "    ret_fill_nan_null = 0.0\n",
    "    if val==True:\n",
    "        ret_fill_nan_null = 1.0\n",
    "    return ret_fill_nan_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(df_data_types)):\n",
    "    df_all[str(df_data_types.index[i])+'_nan_'] = df_all[str(df_data_types.index[i])].map(lambda x:fill_nan_null(pd.isnull(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = df_all.fillna(-9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(df_data_types)):\n",
    "    if(str(df_data_types[i]) == 'object'):\n",
    "        df_u = pd.unique(df_all[str(df_data_types.index[i])].ravel())\n",
    "        print(\"Column: \", str(df_data_types.index[i]), \" Length: \", len(df_u))\n",
    "        d={}\n",
    "        j=1000\n",
    "        for s in df_u:\n",
    "            d[str(s)]=j\n",
    "            j+=5\n",
    "        df_all[str(df_data_types.index[i])+'_vect_']= df_all[str(df_data_types.index[i])].map(lambda x:d[str(x)])\n",
    "        d_col_drops.append(str(df_data_types.index[i]))\n",
    "        if len(df_u)<150:\n",
    "            dummies = pd.get_dummies(df_all[str(df_data_types.index[i])]).rename(columns = lambda x: str(df_data_types.index[i])+str(x))\n",
    "            df_all_temp = pd.concat([df_all_temp,dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all_temp = df_all_temp.drop('ID',axis=1)\n",
    "df_all = pd.concat([df_all,df_all_temp], axis=1)\n",
    "print(len(df_all),len(df_all.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Just in case, we need to start all the things over again\n",
    "#df_all.to_csv(\"df_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Seperate out train and test data to run the model\n",
    "train = df_all.iloc[:num_train]\n",
    "test = df_all.iloc[num_train:]\n",
    "train = train.drop(d_col_drops,axis=1)\n",
    "test = test.drop(d_col_drops,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flog_loss(ground_truth,predictions):\n",
    "    flog_loss_ = log_loss(ground_truth,predictions)\n",
    "    return flog_loss_\n",
    "\n",
    "LL = make_scorer(flog_loss, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g={'ne':150,'md':6,'mf':80,'rs':2016} #change to g={'ne':500,'md':40,'mf':60,'rs':2016}\n",
    "etc = ensemble.ExtraTreesClassifier(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], \n",
    "                                    random_state=g['rs'], criterion='entropy', min_samples_split= 4, \n",
    "                                    min_samples_leaf= 2, verbose = 0, n_jobs =-1)\n",
    "\n",
    "etr = ensemble.ExtraTreesRegressor(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], \n",
    "                                   random_state=g['rs'], min_samples_split= 4, min_samples_leaf= 2,\n",
    "                                   verbose = 0, n_jobs =-1)    \n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], \n",
    "                                      random_state=g['rs'], criterion='entropy', min_samples_split= 4, \n",
    "                                      min_samples_leaf= 2, verbose = 0, n_jobs =-1)\n",
    "\n",
    "rfr = ensemble.RandomForestRegressor(n_estimators=g['ne'], max_depth=g['md'], max_features=g['mf'], \n",
    "                                     random_state=g['rs'], min_samples_split= 4, min_samples_leaf= 2,\n",
    "                                     verbose = 0, n_jobs =-1)\n",
    "\n",
    "xgr = xgb.XGBRegressor(n_estimators=g['ne'], max_depth=g['md'], seed=g['rs'], missing=np.nan, \n",
    "                       learning_rate=0.02, subsample=0.9, colsample_bytree=0.85, \n",
    "                       objective='reg:linear')\n",
    "\n",
    "xgc = xgb.XGBClassifier(n_estimators=g['ne'], max_depth=g['md'], seed=g['rs'], missing=np.nan, \n",
    "                        learning_rate=0.02, subsample=0.9, colsample_bytree=0.85, \n",
    "                        objective='binary:logistic') #try 'binary:logitraw'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clf = {'etc':etc, 'etr':etr, 'rfc':rfc, 'rfr':rfr, 'xgr':xgr, 'xgc':xgc} # use this line instead\n",
    "# removed due to kaggle performance, would prefer less time and more cores than more time and less cores :)\n",
    "clf = {'etr':etr, 'rfr':rfr, 'xgr':xgr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "best_score = 0.0\n",
    "id_results = id_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in clf:\n",
    "    if c[:1] != \"x\": #not xgb\n",
    "        model = GridSearchCV(estimator=clf[c], param_grid={}, n_jobs =-1, cv=2, verbose=0, scoring=LL)\n",
    "        model.fit(train, y_train.values)\n",
    "        if c[-1:] != \"c\": #not classifier\n",
    "            y_pred = model.predict(test)\n",
    "            print(\"Ensemble Model: \", c, \" Best CV score: \", model.best_score_, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "        else: #classifier\n",
    "            best_score = (log_loss(y_train.values, model.predict_proba(train)))*-1\n",
    "            y_pred = model.predict_proba(test)[:,1]\n",
    "            print(\"Ensemble Model: \", c, \" Best CV score: \", best_score, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "    else: #xgb\n",
    "        X_fit, X_eval, y_fit, y_eval= train_test_split(train, y_train, test_size=0.35, train_size=0.65, random_state=g['rs'])\n",
    "        model = clf[c]\n",
    "        model.fit(X_fit, y_fit.values, early_stopping_rounds=20, eval_metric=\"logloss\", eval_set=[(X_eval, y_eval)], verbose=0)\n",
    "        if c == \"xgr\": #xgb regressor\n",
    "            best_score = (log_loss(y_train.values, model.predict(train)))*-1\n",
    "            y_pred = model.predict(test)\n",
    "        else: #xgb classifier\n",
    "            best_score = (log_loss(y_train.values, model.predict_proba(train)))*-1\n",
    "            y_pred = model.predict_proba(test)[:,1]\n",
    "        print(\"Ensemble Model: \", c, \" Best CV score: \", best_score, \" Time: \", round(((time.time() - start_time)/60),2))\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i]<0.0:\n",
    "                y_pred[i] = 0.0\n",
    "            if y_pred[i]>1.0:\n",
    "                y_pred[i] = 1.0\n",
    "        df_in = pd.DataFrame({\"ID\": id_test, c: y_pred})\n",
    "        id_results = pd.concat([id_results, df_in[c]], axis=1)\n",
    "        \n",
    "id_results['avg'] = id_results.drop('ID', axis=1).apply(np.average, axis=1)\n",
    "id_results['min'] = id_results.drop('ID', axis=1).apply(min, axis=1)\n",
    "id_results['max'] = id_results.drop('ID', axis=1).apply(max, axis=1)\n",
    "id_results['diff'] = id_results['max'] - id_results['min']\n",
    "for i in range(10):\n",
    "    print(i, len(id_results[id_results['diff']>(i/10)]))\n",
    "id_results.to_csv(\"results_analysis.csv\", index=False)\n",
    "ds = id_results[['ID','avg']]\n",
    "ds.columns = ['ID','PredictedProb']\n",
    "ds.to_csv('submission_xgboost.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
